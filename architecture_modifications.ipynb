{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0675df25",
   "metadata": {},
   "source": [
    "# Object Detection Architecture Modifications Report\n",
    "\n",
    "**Student:** Darshan  \n",
    "**Dataset:** Waymo COCO (10,000 images)  \n",
    "**Base Model:** Faster R-CNN with ResNet152 Backbone  \n",
    "\n",
    "---\n",
    "\n",
    "## 1. Baseline Model\n",
    "\n",
    "### Architecture Overview\n",
    "- **Backbone:** ResNet152 (pretrained on ImageNet)\n",
    "- **Neck:** Feature Pyramid Network (FPN)\n",
    "- **Head:** Faster R-CNN with RPN\n",
    "- **Classes:** 4 (Vehicle, Pedestrian, Cyclist, Sign)\n",
    "- **Parameters:** 76M total, 17.8M trainable\n",
    "\n",
    "### Training Configuration\n",
    "```python\n",
    "BASELINE_CONFIG = {\n",
    "    'batch_size': 32,\n",
    "    'epochs': 30,\n",
    "    'lr': 0.01,\n",
    "    'lr_schedule': 'MultiStepLR (drop at epoch 20)',\n",
    "    'optimizer': 'SGD (momentum=0.9)',\n",
    "    'train_images': 9000,\n",
    "    'val_images': 1000\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80716fe3",
   "metadata": {},
   "source": [
    "## 2. Proposed Architecture Modifications\n",
    "\n",
    "### Modification 1: CBAM (Convolutional Block Attention Module)\n",
    "\n",
    "**Motivation:**  \n",
    "Standard CNNs treat all channels and spatial locations equally. CBAM adds attention mechanisms to focus on:\n",
    "- **Channel Attention:** Which feature channels are most important?\n",
    "- **Spatial Attention:** Which spatial locations contain objects?\n",
    "\n",
    "**Implementation:**  \n",
    "Insert CBAM blocks after ResNet bottleneck layers in layer3 and layer4.\n",
    "\n",
    "```python\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        # Channel attention\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(channels // reduction, channels)\n",
    "        )\n",
    "        # Spatial attention\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=7, padding=3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Channel attention\n",
    "        avg_out = self.fc(self.avg_pool(x).view(x.size(0), -1))\n",
    "        max_out = self.fc(self.max_pool(x).view(x.size(0), -1))\n",
    "        channel_att = torch.sigmoid(avg_out + max_out).unsqueeze(2).unsqueeze(3)\n",
    "        x = x * channel_att\n",
    "        \n",
    "        # Spatial attention\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        spatial_att = torch.sigmoid(self.conv(torch.cat([avg_out, max_out], dim=1)))\n",
    "        x = x * spatial_att\n",
    "        \n",
    "        return x\n",
    "```\n",
    "\n",
    "**Expected Improvement:** +2-4% mAP  \n",
    "**Computational Overhead:** ~5% increase in training time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5daa13",
   "metadata": {},
   "source": [
    "### Modification 2: Enhanced FPN with Bottom-Up Path Augmentation (PANet-style)\n",
    "\n",
    "**Motivation:**  \n",
    "Standard FPN has top-down pathway only. Low-level features (fine details) take a long path to reach high-level predictions.\n",
    "\n",
    "**Implementation:**  \n",
    "Add bottom-up path augmentation to shorten information path from low-level to high-level features.\n",
    "\n",
    "```\n",
    "Standard FPN:           Enhanced FPN:\n",
    "C5 → P5                 C5 → P5 ←→ N5\n",
    " ↓    ↓                  ↓    ↓    ↑\n",
    "C4 → P4                 C4 → P4 ←→ N4\n",
    " ↓    ↓                  ↓    ↓    ↑\n",
    "C3 → P3                 C3 → P3 ←→ N3\n",
    " ↓    ↓                  ↓    ↓    ↑\n",
    "C2 → P2                 C2 → P2 ←→ N2\n",
    "```\n",
    "\n",
    "**Expected Improvement:** +3-5% mAP (especially for small objects)  \n",
    "**Computational Overhead:** ~10% increase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93404775",
   "metadata": {},
   "source": [
    "### Modification 3: Multi-Scale Training\n",
    "\n",
    "**Motivation:**  \n",
    "Objects appear at different scales in autonomous driving scenes. Training with multiple scales improves scale invariance.\n",
    "\n",
    "**Implementation:**  \n",
    "Randomly sample input resolution from [640, 720, 800, 960] during training.\n",
    "\n",
    "**Expected Improvement:** +2-3% mAP  \n",
    "**Computational Overhead:** Minimal (same as baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d2ac33",
   "metadata": {},
   "source": [
    "## 3. Experimental Setup\n",
    "\n",
    "### Experiments to Run\n",
    "\n",
    "| Exp ID | Model | Description | Expected mAP@0.5 |\n",
    "|--------|-------|-------------|------------------|\n",
    "| **E0** | Baseline | ResNet152 + Faster R-CNN | 0.40-0.50 |\n",
    "| **E1** | + CBAM | Add attention to backbone | 0.43-0.53 |\n",
    "| **E2** | + Enhanced FPN | Add bottom-up augmentation | 0.44-0.55 |\n",
    "| **E3** | + CBAM + Enhanced FPN | Combined modifications | 0.46-0.57 |\n",
    "| **E4** | + All + Multi-scale | Final optimized model | 0.48-0.59 |\n",
    "\n",
    "### Training Commands\n",
    "\n",
    "```bash\n",
    "# E0: Baseline (already training)\n",
    "--expname 'baseline_resnet152' --model 'customrcnn_resnet152'\n",
    "\n",
    "# E1: Baseline + CBAM\n",
    "--expname 'resnet152_cbam' --model 'customrcnn_resnet152_cbam'\n",
    "\n",
    "# E2: Baseline + Enhanced FPN\n",
    "--expname 'resnet152_enhancedfpn' --model 'customrcnn_resnet152_enhancedfpn'\n",
    "\n",
    "# E3: Combined\n",
    "--expname 'resnet152_cbam_enhancedfpn' --model 'customrcnn_resnet152_cbam_enhancedfpn'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d20395",
   "metadata": {},
   "source": [
    "## 4. Evaluation Metrics\n",
    "\n",
    "### Primary Metrics (COCO Format)\n",
    "- **AP@[0.5:0.95]:** Average Precision at IoU thresholds 0.5 to 0.95\n",
    "- **AP@0.5:** Average Precision at IoU=0.5 (primary comparison metric)\n",
    "- **AP@0.75:** Average Precision at IoU=0.75 (strict localization)\n",
    "- **AR@100:** Average Recall with 100 detections per image\n",
    "\n",
    "### Per-Class Metrics\n",
    "- Vehicle AP\n",
    "- Pedestrian AP\n",
    "- Cyclist AP\n",
    "- Sign AP\n",
    "\n",
    "### Scale-wise Metrics\n",
    "- Small objects (area < 32²)\n",
    "- Medium objects (32² < area < 96²)\n",
    "- Large objects (area > 96²)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48658ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for result visualization\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441d9432",
   "metadata": {},
   "source": [
    "## 5. Results Collection\n",
    "\n",
    "### Baseline Results (E0)\n",
    "\n",
    "Training started: [DATE]  \n",
    "Training completed: [DATE]  \n",
    "Total training time: [TIME]\n",
    "\n",
    "**Final Metrics:**\n",
    "```\n",
    "AP@[0.5:0.95] = [TO BE FILLED]\n",
    "AP@0.5 = [TO BE FILLED]\n",
    "AP@0.75 = [TO BE FILLED]\n",
    "AR@100 = [TO BE FILLED]\n",
    "```\n",
    "\n",
    "**Per-Class AP@0.5:**\n",
    "- Vehicle: [TO BE FILLED]\n",
    "- Pedestrian: [TO BE FILLED]\n",
    "- Cyclist: [TO BE FILLED]\n",
    "- Sign: [TO BE FILLED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be86c656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for results DataFrame\n",
    "results = {\n",
    "    'Model': ['Baseline', 'CBAM', 'Enhanced FPN', 'CBAM+FPN', 'All+Multiscale'],\n",
    "    'AP@0.5': [0.0, 0.0, 0.0, 0.0, 0.0],  # TO BE FILLED\n",
    "    'AP@0.75': [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    'AP@[0.5:0.95]': [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    'AR@100': [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    'Training Time (hrs)': [3.0, 3.2, 3.3, 3.5, 3.6]\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"Results Summary:\")\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b86dc4f",
   "metadata": {},
   "source": [
    "## 6. Performance Comparison Visualizations\n",
    "\n",
    "### 6.1 Overall mAP Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe133197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot: mAP comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# AP@0.5 comparison\n",
    "df_results.plot(x='Model', y='AP@0.5', kind='bar', ax=ax1, color='steelblue', legend=False)\n",
    "ax1.set_title('Average Precision @ IoU=0.5', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('AP@0.5', fontsize=12)\n",
    "ax1.set_xlabel('Model Architecture', fontsize=12)\n",
    "ax1.set_ylim([0, 0.7])\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# AP@[0.5:0.95] comparison\n",
    "df_results.plot(x='Model', y='AP@[0.5:0.95]', kind='bar', ax=ax2, color='coral', legend=False)\n",
    "ax2.set_title('Average Precision @ IoU=0.5:0.95', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('AP@[0.5:0.95]', fontsize=12)\n",
    "ax2.set_xlabel('Model Architecture', fontsize=12)\n",
    "ax2.set_ylim([0, 0.5])\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('mAP_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Saved: mAP_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03daf7e8",
   "metadata": {},
   "source": [
    "### 6.2 Per-Class Performance Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac18e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class AP data (to be filled after experiments)\n",
    "per_class_data = {\n",
    "    'Model': ['Baseline']*4 + ['CBAM']*4 + ['Enhanced FPN']*4 + ['CBAM+FPN']*4,\n",
    "    'Class': ['Vehicle', 'Pedestrian', 'Cyclist', 'Sign']*4,\n",
    "    'AP@0.5': [0.0]*16  # TO BE FILLED\n",
    "}\n",
    "\n",
    "df_perclass = pd.DataFrame(per_class_data)\n",
    "\n",
    "# Grouped bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "df_pivot = df_perclass.pivot(index='Class', columns='Model', values='AP@0.5')\n",
    "df_pivot.plot(kind='bar', ax=ax, width=0.8)\n",
    "ax.set_title('Per-Class AP@0.5 Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('AP@0.5', fontsize=12)\n",
    "ax.set_xlabel('Object Class', fontsize=12)\n",
    "ax.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('per_class_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Saved: per_class_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a70dc51",
   "metadata": {},
   "source": [
    "### 6.3 Training Efficiency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f66c1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: mAP vs Training Time\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "scatter = ax.scatter(df_results['Training Time (hrs)'], \n",
    "                     df_results['AP@0.5'], \n",
    "                     s=200, alpha=0.6, c=range(len(df_results)), \n",
    "                     cmap='viridis', edgecolors='black', linewidth=1.5)\n",
    "\n",
    "# Add labels for each point\n",
    "for idx, row in df_results.iterrows():\n",
    "    ax.annotate(row['Model'], \n",
    "                (row['Training Time (hrs)'], row['AP@0.5']),\n",
    "                xytext=(5, 5), textcoords='offset points',\n",
    "                fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Training Time (hours)', fontsize=12)\n",
    "ax.set_ylabel('AP@0.5', fontsize=12)\n",
    "ax.set_title('Model Performance vs Training Time Trade-off', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('efficiency_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Saved: efficiency_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a463bea3",
   "metadata": {},
   "source": [
    "## 7. Analysis & Discussion\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **CBAM Attention Impact:**\n",
    "   - [TO BE FILLED after experiments]\n",
    "   - Most effective for: [class/scale]\n",
    "   - Computational cost: acceptable/high\n",
    "\n",
    "2. **Enhanced FPN Impact:**\n",
    "   - [TO BE FILLED]\n",
    "   - Small object detection: improved/similar\n",
    "   - Best for: [scenario]\n",
    "\n",
    "3. **Combined Modifications:**\n",
    "   - [TO BE FILLED]\n",
    "   - Synergistic effects: observed/not observed\n",
    "   - Final recommendation: [model choice]\n",
    "\n",
    "### Challenges & Solutions\n",
    "\n",
    "- **Challenge 1:** [TO BE FILLED]\n",
    "  - **Solution:** [TO BE FILLED]\n",
    "\n",
    "- **Challenge 2:** [TO BE FILLED]\n",
    "  - **Solution:** [TO BE FILLED]\n",
    "\n",
    "### Future Improvements\n",
    "\n",
    "1. Test with larger datasets (full Waymo/nuScenes)\n",
    "2. Implement Cascade R-CNN for better localization\n",
    "3. Try Deformable DETR for end-to-end training\n",
    "4. Experiment with Vision Transformer backbones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069082de",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "[TO BE FILLED after completing all experiments]\n",
    "\n",
    "Summary of improvements:\n",
    "- Baseline mAP@0.5: [X.XX]\n",
    "- Best modified model: [MODEL NAME]\n",
    "- Best mAP@0.5: [X.XX]\n",
    "- Improvement: +[X.X]% relative gain\n",
    "\n",
    "**Selected model for deployment:** [MODEL NAME]  \n",
    "**Justification:** [REASONING]\n",
    "\n",
    "---\n",
    "\n",
    "## 9. References\n",
    "\n",
    "1. Ren, S., et al. (2015). \"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.\" NeurIPS.\n",
    "2. Lin, T. Y., et al. (2017). \"Feature Pyramid Networks for Object Detection.\" CVPR.\n",
    "3. Woo, S., et al. (2018). \"CBAM: Convolutional Block Attention Module.\" ECCV.\n",
    "4. Liu, S., et al. (2018). \"Path Aggregation Network for Instance Segmentation.\" CVPR.\n",
    "5. He, K., et al. (2016). \"Deep Residual Learning for Image Recognition.\" CVPR.\n",
    "\n",
    "---\n",
    "\n",
    "## Source Code Repository\n",
    "\n",
    "**GitHub:** https://github.com/Darshanhub/DeepDataMiningPersonal  \n",
    "**Training Scripts:** `DeepDataMiningLearning/detection/mytrain.py`  \n",
    "**Model Definitions:** `DeepDataMiningLearning/detection/models.py`  \n",
    "**This Report:** `architecture_modifications.ipynb`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
